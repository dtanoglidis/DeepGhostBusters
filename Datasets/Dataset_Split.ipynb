{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "twenty-myanmar",
   "metadata": {},
   "source": [
    "## Train-Validation-Test set split\n",
    "\n",
    "In this notebook, I split the full dataset into training, validation, and test sets.\n",
    "\n",
    "\n",
    "This split is performed both on the images and the annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "powered-boards",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import basic packages\n",
    "import numpy as np \n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "import skimage.draw\n",
    "import numpy.ma as ma\n",
    "import imageio\n",
    "\n",
    "# Train test split\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assisted-checklist",
   "metadata": {},
   "source": [
    "### Import image data \n",
    "\n",
    "Let's first import the images - these are stored are numpy arrays.\n",
    "They come from Mike Wang's training set for the paper \"A Machine Learning Approach to the Detection of Ghosting Artifacts in Dark Energy Survey Images\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fourth-taste",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_temp = np.load(\"train_set/x_ghstcln400-cnn5_ins4.npy\")/255.0 #Images\n",
    "y_train_temp = np.load(\"train_set/y_ghstcln400-cnn5_fix9z0_ins4.npy\")#Labels\n",
    "z_train_temp = np.load(\"train_set/z_ghstcln400-cnn5_ins4.npy\")#contains expnum, year, and filter for each image in \"x\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinguished-athletics",
   "metadata": {},
   "source": [
    "Keep only the ghosts (those with label = 1); get the exposure numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "pressing-charlotte",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2499\n"
     ]
    }
   ],
   "source": [
    "X_train_ghosts = X_train_temp[(y_train_temp==1.0)]\n",
    "z_train_ghosts = z_train_temp[(y_train_temp==1.0)]\n",
    "Expnums = z_train_ghosts['expnum']\n",
    "print(len(Expnums))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deluxe-salon",
   "metadata": {},
   "source": [
    "For the training set we used we also demand the ghosts to have been classified as such by the neural network (mentioned in the above paper) with probability > $95\\%$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sudden-future",
   "metadata": {},
   "source": [
    "### Import annotations\n",
    "\n",
    "Now, let's import the `json` file containing the annotations for the full dataset of 2000 images we used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "technical-cartoon",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_full = json.load(open(\"annotations_full.json\"))\n",
    "annotations_file = list(annotations_full['_via_img_metadata'].values())  # don't need the dict keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "sound-process",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000,)\n"
     ]
    }
   ],
   "source": [
    "# Let's see its size, it should be 2000\n",
    "print(np.shape(annotations_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "private-advocacy",
   "metadata": {},
   "source": [
    "Let's start by getting the filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "trained-exclusive",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get filenames \n",
    "filenames = []\n",
    "\n",
    "for i in range(2000):\n",
    "    filename_loc = annotations_file[i]['filename']\n",
    "    filenames.append(filename_loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "molecular-catholic",
   "metadata": {},
   "source": [
    "Get all the annotations as a dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "social-academy",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = [a for a in annotations_file]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "actual-birthday",
   "metadata": {},
   "source": [
    "### Split now both annotations and images\n",
    "\n",
    "Now I have to split the annotations and images into a training-validation and test sets.\n",
    "\n",
    "To do that, I will simply create an array with integer entries 1-2000 and split that in $70\\%$ for training, $15\\%$ validation, and $15\\%$ test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "liberal-nevada",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_full = np.arange(2000)\n",
    "\n",
    "# Split into training and validation-test (combided)\n",
    "indices_train, indices_valtest = train_test_split(indices_full, test_size=0.3, random_state=42)\n",
    "\n",
    "# Split validation-test sets\n",
    "indices_val, indices_test = train_test_split(indices_valtest, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resident-makeup",
   "metadata": {},
   "source": [
    "As a sanity check, let's print their size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "synthetic-toolbox",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1400 300 300\n"
     ]
    }
   ],
   "source": [
    "print(len(indices_train),len(indices_val),len(indices_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "derived-ideal",
   "metadata": {},
   "source": [
    "#### Split and save annotation json files\n",
    "\n",
    "Now let's split the annotations (dictionaries) to training/validation/test sets and save them to corresponding `json` files.\n",
    "\n",
    "- For training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "polished-oxford",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Array containing the filenames for the training set\n",
    "filenames_train = np.array(filenames)[indices_train] \n",
    "# Now for the annotations of the training set\n",
    "annotations_train = []\n",
    "for i in range(len(indices_train)):\n",
    "    local_annot = annotations[indices_train[i]]\n",
    "    annotations_train.append(local_annot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "distinguished-sheet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save into a json file\n",
    "json_annot_train = json.dumps(annotations_train)\n",
    "f = open(\"annotations_train.json\",\"w\")\n",
    "f.write(json_annot_train)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demanding-effort",
   "metadata": {},
   "source": [
    "- For validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "alone-ocean",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Array containing the filenames for the validation set\n",
    "filenames_val = np.array(filenames)[indices_val] \n",
    "# Now for the annotations of the validation set\n",
    "annotations_val = []\n",
    "for i in range(len(indices_val)):\n",
    "    local_annot = annotations[indices_val[i]]\n",
    "    annotations_val.append(local_annot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "demanding-cholesterol",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save into a json file\n",
    "json_annot_val = json.dumps(annotations_val)\n",
    "f = open(\"annotations_val.json\",\"w\")\n",
    "f.write(json_annot_val)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "permanent-myanmar",
   "metadata": {},
   "source": [
    "- For test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "skilled-halloween",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Array containing the filenames for the test set\n",
    "filenames_test = np.array(filenames)[indices_test] \n",
    "# Now for the annotations of the test set\n",
    "annotations_test = []\n",
    "for i in range(len(indices_test)):\n",
    "    local_annot = annotations[indices_test[i]]\n",
    "    annotations_test.append(local_annot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "clinical-dictionary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save into a json file\n",
    "json_annot_test = json.dumps(annotations_test)\n",
    "f = open(\"annotations_test.json\",\"w\")\n",
    "f.write(json_annot_test)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entertaining-microwave",
   "metadata": {},
   "source": [
    "### Now split the images\n",
    "\n",
    "- For the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "super-proof",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D00589715\n"
     ]
    }
   ],
   "source": [
    "# Get exposure numbers\n",
    "Expnum_train = []\n",
    "\n",
    "for i in range(len(indices_train)):\n",
    "    filename_loc = filenames_train[i]\n",
    "    expnum_loc = filename_loc[10:19]\n",
    "    Expnum_train.append(expnum_loc)\n",
    "    \n",
    "    \n",
    "print(Expnum_train[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bulgarian-franklin",
   "metadata": {},
   "source": [
    "Now, populate the `Train_set` file with the corresponding images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "annoying-command",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(indices_train)\n",
    "\n",
    "for i in range(n):\n",
    "    # Get the exposure number for the i-th element of training set\n",
    "    Expnum_loc = Expnum_train[i]\n",
    "    # Local ghost \n",
    "    X_loc = X_train_ghosts[Expnums==Expnum_loc][0,:,:]\n",
    "    # ================================================\n",
    "    # ================================================\n",
    "    # Initialize\n",
    "    leng = np.shape(X_loc)[0] #Size of image (400pixels)\n",
    "    # For three channels\n",
    "    X_ghost_3ch = np.zeros((leng,leng,3))\n",
    "\n",
    "    #Populate \n",
    "    X_ghost_3ch[:,:,0] = X_loc\n",
    "    X_ghost_3ch[:,:,1] = X_loc\n",
    "    X_ghost_3ch[:,:,2] = X_loc\n",
    "    \n",
    "    # Save the image\n",
    "    #imageio.imwrite(\"./Training_set/Ghost_img_{0}.jpg\".format(Expnum_loc),(X_ghost_3ch*255.).astype(np.uint8))   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dietary-communications",
   "metadata": {},
   "source": [
    "- For the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "relative-dutch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D00577759\n"
     ]
    }
   ],
   "source": [
    "# Get exposure numbers\n",
    "Expnum_val = []\n",
    "\n",
    "for i in range(len(indices_val)):\n",
    "    filename_loc = filenames_val[i]\n",
    "    expnum_loc = filename_loc[10:19]\n",
    "    Expnum_val.append(expnum_loc)\n",
    "    \n",
    "    \n",
    "print(Expnum_val[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "greek-multiple",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(indices_val)\n",
    "\n",
    "for i in range(n):\n",
    "    # Get the exposure number for the i-th element of training set\n",
    "    Expnum_loc = Expnum_val[i]\n",
    "    # Local ghost \n",
    "    X_loc = X_train_ghosts[Expnums==Expnum_loc][0,:,:] #Note, train here is general\n",
    "    # Refers to the initial training set\n",
    "    # ================================================\n",
    "    # ================================================\n",
    "    # Initialize\n",
    "    leng = np.shape(X_loc)[0] #Size of image (400pixels)\n",
    "    # For three channels\n",
    "    X_ghost_3ch = np.zeros((leng,leng,3))\n",
    "\n",
    "    #Populate \n",
    "    X_ghost_3ch[:,:,0] = X_loc\n",
    "    X_ghost_3ch[:,:,1] = X_loc\n",
    "    X_ghost_3ch[:,:,2] = X_loc\n",
    "    \n",
    "    # Save the image\n",
    "    #imageio.imwrite(\"./Validation_set/Ghost_img_{0}.jpg\".format(Expnum_loc),(X_ghost_3ch*255.).astype(np.uint8))   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpine-production",
   "metadata": {},
   "source": [
    "- For the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "operating-madrid",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D00262304\n"
     ]
    }
   ],
   "source": [
    "# Get exposure numbers\n",
    "Expnum_test = []\n",
    "\n",
    "for i in range(len(indices_test)):\n",
    "    filename_loc = filenames_test[i]\n",
    "    expnum_loc = filename_loc[10:19]\n",
    "    Expnum_test.append(expnum_loc)\n",
    "    \n",
    "    \n",
    "print(Expnum_test[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "ready-estate",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(indices_test)\n",
    "\n",
    "for i in range(n):\n",
    "    # Get the exposure number for the i-th element of training set\n",
    "    Expnum_loc = Expnum_test[i]\n",
    "    # Local ghost \n",
    "    X_loc = X_train_ghosts[Expnums==Expnum_loc][0,:,:] #Note, train here is general\n",
    "    # Refers to the initial training set\n",
    "    # ================================================\n",
    "    # ================================================\n",
    "    # Initialize\n",
    "    leng = np.shape(X_loc)[0] #Size of image (400pixels)\n",
    "    # For three channels\n",
    "    X_ghost_3ch = np.zeros((leng,leng,3))\n",
    "\n",
    "    #Populate \n",
    "    X_ghost_3ch[:,:,0] = X_loc\n",
    "    X_ghost_3ch[:,:,1] = X_loc\n",
    "    X_ghost_3ch[:,:,2] = X_loc\n",
    "    \n",
    "    # Save the image\n",
    "    #imageio.imwrite(\"./Test_set/Ghost_img_{0}.jpg\".format(Expnum_loc),(X_ghost_3ch*255.).astype(np.uint8))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immune-beauty",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
